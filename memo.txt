spark-submit /home/vagrant/work/spark_job/read_s3_data.py s3n://bigdatasimplestorage/data/pg38508.txt ${YOUR_AWS_ACCESS_KEY_ID} ${YOUR_AWS_SECRET_ACCESS_KEY}


spark-submit --py-files /home/vagrant/work/spark_job/read_s3_data.py /home/vagrant/work/spark_job/read_s3_data_slave.py s3n://bigdatasimplestorage/data/pg38508.txt ${YOUR_AWS_ACCESS_KEY_ID} ${YOUR_AWS_SECRET_ACCESS_KEY}



spark-submit /home/vagrant/work/spark_job/read_s3_data.zip \
s3n://bigdatasimplestorage/data/pg38508.txt \
${YOUR_AWS_ACCESS_KEY_ID} \
${YOUR_AWS_SECRET_ACCESS_KEY}


{"uri": "s3n://bigdatasimplestorage/data/pg38508.txt",
 "AWS_SECRET_ACCESS_KEY":"${YOUR_AWS_SECRET_ACCESS_KEY}",
 "AWS_ACCESS_KEY_ID":"${YOUR_AWS_SECRET_ACCESS_KEY}"

'uri'],
	args['AWS_ACCESS_KEY_ID'],
	args['AWS_SECRET_ACCESS_KEY']
   ]


text_file = sc.textFile("README.md")

counts = text_file.flatMap(lambda line: linete.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

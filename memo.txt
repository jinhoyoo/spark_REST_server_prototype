spark-submit /home/vagrant/work/spark_job/read_s3_data.py s3n://bigdatasimplestorage/data/pg38508.txt AKIAIVNDU45MGS7ORL3A UmZ6DOBIxOdgAw2avG4bfiTrXKKluymscgjz3r8n


spark-submit --py-files /home/vagrant/work/spark_job/read_s3_data.py /home/vagrant/work/spark_job/read_s3_data_slave.py s3n://bigdatasimplestorage/data/pg38508.txt AKIAIVNDU45MGS7ORL3A UmZ6DOBIxOdgAw2avG4bfiTrXKKluymscgjz3r8n


spark-submit /home/vagrant/work/spark_job/read_s3_data.zip \
s3n://bigdatasimplestorage/data/pg38508.txt \
AKIAIVNDU45MGS7ORL3A \
UmZ6DOBIxOdgAw2avG4bfiTrXKKluymscgjz3r8n


{"uri": "s3n://bigdatasimplestorage/data/pg38508.txt",
 "AWS_SECRET_ACCESS_KEY":"UmZ6DOBIxOdgAw2avG4bfiTrXKKluymscgjz3r8n",
 "AWS_ACCESS_KEY_ID":"AKIAIVNDU45MGS7ORL3A"

'uri'],
	args['AWS_ACCESS_KEY_ID'],
	args['AWS_SECRET_ACCESS_KEY']
   ]


text_file = sc.textFile("README.md")

counts = text_file.flatMap(lambda line: linete.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
